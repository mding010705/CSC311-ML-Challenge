{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "wOHEGXTtMcdU",
        "outputId": "55491072-ee8d-46df-8a96-c386f5e7ef45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1644, 519)\n",
            "(1644, 519)\n",
            "(1644,)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "string index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-090e6587a500>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/clean_results_test.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0mpredict_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-090e6587a500>\u001b[0m in \u001b[0;36mpredict_all\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_cleaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m     \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m     \u001b[0mfinal_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-090e6587a500>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m         \u001b[0mtree_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m         return np.array(\n\u001b[1;32m    480\u001b[0m             [most_common_label(tree_preds[:, i]) for i in range(X.shape[0])])\n",
            "\u001b[0;32m<ipython-input-19-090e6587a500>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m         \u001b[0mtree_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m         return np.array(\n\u001b[1;32m    480\u001b[0m             [most_common_label(tree_preds[:, i]) for i in range(X.shape[0])])\n",
            "\u001b[0;32m<ipython-input-19-090e6587a500>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-090e6587a500>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-090e6587a500>\u001b[0m in \u001b[0;36mpredict_one\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m             node = node.left if x[\n\u001b[0m\u001b[1;32m    447\u001b[0m                                     node.feature_index] <= node.threshold else node.right\n\u001b[1;32m    448\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: string index out of range"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import sys\n",
        "import re\n",
        "import time\n",
        "\n",
        "stop_words = set(\n",
        "    [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"ain\", \"all\",\n",
        "     \"am\", \"an\", \"and\", \"any\", \"are\", \"aren\", \"aren't\", \"as\", \"at\", \"be\",\n",
        "     \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\",\n",
        "     \"but\", \"by\", \"can\", \"couldn\", \"couldn't\", \"d\", \"did\", \"didn\", \"didn't\",\n",
        "     \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"don't\", \"down\",\n",
        "     \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn\",\n",
        "     \"hadn't\", \"has\", \"hasn\", \"hasn't\", \"have\", \"haven\", \"haven't\",\n",
        "     \"having\", \"he\", \"he'd\", \"he'll\", \"her\", \"here\", \"hers\", \"herself\",\n",
        "     \"he's\", \"him\", \"himself\", \"his\", \"how\", \"i\", \"i'd\", \"if\", \"i'll\",\n",
        "     \"i'm\", \"in\", \"into\", \"is\", \"isn\", \"isn't\", \"it\", \"it'd\", \"it'll\",\n",
        "     \"it's\", \"its\", \"itself\", \"i've\", \"just\", \"ll\", \"m\", \"ma\", \"me\",\n",
        "     \"mightn\", \"mightn't\", \"more\", \"most\", \"mustn\", \"mustn't\", \"my\",\n",
        "     \"myself\", \"needn\", \"needn't\", \"no\", \"nor\", \"not\", \"now\", \"o\", \"of\",\n",
        "     \"off\", \"on\", \"once\", \"only\", \"or\", \"other\", \"our\", \"ours\", \"ourselves\",\n",
        "     \"out\", \"over\", \"own\", \"re\", \"s\", \"same\", \"shan\", \"shan't\", \"she\", \"she'd\",\n",
        "     \"she'll\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"so\",\n",
        "     \"some\", \"such\", \"t\", \"than\", \"that\", \"that'll\", \"the\", \"their\", \"theirs\",\n",
        "     \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"they'd\",\n",
        "     \"they'll\",\n",
        "     \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\",\n",
        "     \"until\", \"up\", \"ve\", \"very\", \"was\", \"wasn\", \"wasn't\", \"we\", \"we'd\",\n",
        "     \"we'll\", \"we're\", \"were\", \"weren\", \"weren't\", \"we've\", \"what\", \"when\",\n",
        "     \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"won\",\n",
        "     \"won't\", \"wouldn\", \"wouldn't\", \"y\", \"you\", \"you'd\", \"you'll\", \"your\",\n",
        "     \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"\"])\n",
        "sub_imdb = pd.read_csv(\"/tmdb_movies_tv.csv\")\n",
        "sub_imdb['title'] = sub_imdb['title'].apply(str)\n",
        "\n",
        "\n",
        "# --- Helper Functions for Data Cleaning ---\n",
        "\n",
        "def change_movie(movie: str):\n",
        "    return re.sub(r'[^\\w\\s]', '',\n",
        "                  str(movie).lower().removeprefix(\"the \").removeprefix(\n",
        "                      \"a \").removeprefix(\"an \")).replace(\" \", \"\")[:18]\n",
        "\n",
        "\n",
        "def detect_num_of_ingredients(text: str) -> list[str]:\n",
        "    text = str(text)\n",
        "    numbers = re.findall(r\"\\d+\", str(text))\n",
        "    numbers = [int(num) for num in numbers]\n",
        "\n",
        "    number_words = [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\",\n",
        "                    \"eight\", \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\",\n",
        "                    \"fourteen\", \"fifteen\", \"sixteen\", \"seventeen\", \"eighteen\",\n",
        "                    \"nineteen\", \"twenty\"]\n",
        "\n",
        "    word_map = {\"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5, \"six\": 6,\n",
        "                \"seven\": 7,\n",
        "                \"eight\": 8, \"nine\": 9, \"ten\": 10, \"eleven\": 11, \"twelve\": 12,\n",
        "                \"thirteen\": 13,\n",
        "                \"fourteen\": 14, \"fifteen\": 15, \"sixteen\": 16, \"seventeen\": 17,\n",
        "                \"eighteen\": 18,\n",
        "                \"nineteen\": 19, \"twenty\": 20}\n",
        "\n",
        "    numbers += [word_map[word] for word in number_words if word in text.lower()]\n",
        "\n",
        "    if numbers == []:\n",
        "        ingredients_list = re.split(r\",\", text)\n",
        "        return [len(ingredients_list)]\n",
        "\n",
        "    return numbers\n",
        "\n",
        "\n",
        "def detect_price(text: str) -> list[str]:\n",
        "    numbers = re.findall(r\"\\d+(?:\\.\\d+)?\", str(text))\n",
        "\n",
        "    numbers = [float(num) for num in numbers if num != '']\n",
        "\n",
        "    number_words = [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\",\n",
        "                    \"eight\", \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\",\n",
        "                    \"fourteen\", \"fifteen\", \"sixteen\", \"seventeen\", \"eighteen\",\n",
        "                    \"nineteen\", \"twenty\"]\n",
        "\n",
        "    word_map = {\"one\": 1.0, \"two\": 2.0, \"three\": 3.0, \"four\": 4.0, \"five\": 5.0,\n",
        "                \"six\": 6.0, \"seven\": 7.0,\n",
        "                \"eight\": 8.0, \"nine\": 9.0, \"ten\": 10.0, \"eleven\": 11.0,\n",
        "                \"twelve\": 12.0, \"thirteen\": 13.0,\n",
        "                \"fourteen\": 14.0, \"fifteen\": 15.0, \"sixteen\": 16.0,\n",
        "                \"seventeen\": 17.0, \"eighteen\": 18.0,\n",
        "                \"nineteen\": 19.0, \"twenty\": 20.0, \"quarter\": 0.25, \"half\": 0.5,\n",
        "                \"thirty\": 30.0}\n",
        "\n",
        "    numbers += [word_map[word] for word in number_words if\n",
        "                word in str(text).lower()]\n",
        "\n",
        "    if str(text).lower() == \"dollar each\":\n",
        "        numbers = [1.0]\n",
        "\n",
        "    return numbers\n",
        "\n",
        "\n",
        "def clean_ingredients(text: str) -> int:\n",
        "    # Number of ingredients\n",
        "    if not text:\n",
        "        return None\n",
        "    num_ingredients = detect_num_of_ingredients(text)\n",
        "    if num_ingredients:\n",
        "        return sum(num_ingredients) / len(num_ingredients)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def clean_cost(text: str) -> float:\n",
        "    # Expected cost\n",
        "    if not text:\n",
        "        return None\n",
        "    cost = detect_price(text)\n",
        "    if cost:\n",
        "        return sum(cost) / len(cost)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def detect_stopwords(text: str) -> bool:\n",
        "    words = text.lower().split()\n",
        "    return any(word in stop_words for word in words)\n",
        "\n",
        "\n",
        "def detect_capitilized(text: str) -> list[str]:\n",
        "    return re.findall(r'[A-Z][a-zA-Z]*', text)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# from https://github.com/nltk/nltk/blob/develop/nltk/metrics/distance.py\n",
        "def jaro_similarity(s1, s2):\n",
        "    # First, store the length of the strings\n",
        "    # because they will be re-used several times.\n",
        "    len_s1, len_s2 = len(s1), len(s2)\n",
        "\n",
        "    # The upper bound of the distance for being a matched character.\n",
        "    match_bound = max(len_s1, len_s2) // 2 - 1\n",
        "\n",
        "    # Initialize the counts for matches and transpositions.\n",
        "    matches = 0  # no.of matched characters in s1 and s2\n",
        "    transpositions = 0  # no. of transpositions between s1 and s2\n",
        "    flagged_1 = []  # positions in s1 which are matches to some character in s2\n",
        "    flagged_2 = []  # positions in s2 which are matches to some character in s1\n",
        "\n",
        "    # Iterate through sequences, check for matches and compute transpositions.\n",
        "    for i in range(len_s1):  # Iterate through each character.\n",
        "        upperbound = min(i + match_bound, len_s2 - 1)\n",
        "        lowerbound = max(0, i - match_bound)\n",
        "        for j in range(lowerbound, upperbound + 1):\n",
        "            if s1[i] == s2[j] and j not in flagged_2:\n",
        "                matches += 1\n",
        "                flagged_1.append(i)\n",
        "                flagged_2.append(j)\n",
        "                break\n",
        "    flagged_2.sort()\n",
        "    for i, j in zip(flagged_1, flagged_2):\n",
        "        if s1[i] != s2[j]:\n",
        "            transpositions += 1\n",
        "\n",
        "    if matches == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return (\n",
        "                1\n",
        "                / 3\n",
        "                * (\n",
        "                        matches / len_s1\n",
        "                        + matches / len_s2\n",
        "                        + (matches - transpositions // 2) / matches\n",
        "                )\n",
        "        )\n",
        "\n",
        "\n",
        "def jaro_winkler_similarity(s1, s2, p=0.1, max_l=4):\n",
        "    # To ensure that the output of the Jaro-Winkler's similarity\n",
        "    # falls between [0,1], the product of l * p needs to be\n",
        "    # also fall between [0,1].\n",
        "    # Compute the Jaro similarity\n",
        "    jaro_sim = jaro_similarity(s1, s2)\n",
        "\n",
        "    # Initialize the upper bound for the no. of prefixes.\n",
        "    # if user did not pre-define the upperbound,\n",
        "    # use shorter length between s1 and s2\n",
        "\n",
        "    # Compute the prefix matches.\n",
        "    l = 0\n",
        "    # zip() will automatically loop until the end of shorter string.\n",
        "    for s1_i, s2_i in zip(s1, s2):\n",
        "        if s1_i == s2_i:\n",
        "            l += 1\n",
        "        else:\n",
        "            break\n",
        "        if l == max_l:\n",
        "            break\n",
        "    # Return the similarity value as described in docstring.\n",
        "    return jaro_sim + (l * p * (1 - jaro_sim))\n",
        "\n",
        "\n",
        "# ===========================================================================\n",
        "\n",
        "def detect_movie(text: str) -> list[str]:\n",
        "    if not text:\n",
        "        return []\n",
        "    search = sub_imdb.loc[sub_imdb['title'] == text]\n",
        "    if search.empty:\n",
        "        sims = [0] * len(sub_imdb['title'])\n",
        "        sims = sub_imdb['title'].apply(jaro_similarity, s2=text)\n",
        "        search = sub_imdb.iloc[[np.argmax(sims)]]\n",
        "\n",
        "    return search\n",
        "\n",
        "\n",
        "def detect_drinks(text: str) -> list[str]:\n",
        "    if not text:\n",
        "        return []\n",
        "    text = str(text)\n",
        "    common_drinks = {'7up': ['fruit', 'soda'], 'coca-cola': ['cola', 'soda'],\n",
        "                     'iced tea': ['tea', 'soda'], 'pepsi': ['cola', 'soda'],\n",
        "                     'fanta': ['fruit', 'soda'],\n",
        "                     'mirinda': ['fruit', 'soda'],\n",
        "                     'orange juice': ['fruit', 'juice'],\n",
        "                     'ginger ale': ['ginger', 'soda'],\n",
        "                     'lemonade': ['fruit', 'juice'],\n",
        "                     'sprite': ['fruit', 'soda'],\n",
        "                     'kombucha': ['ginger', 'tea'], 'water': ['water'],\n",
        "                     'lemon water': ['fruit', 'water'], 'pop': ['soda'],\n",
        "                     'diet coke': ['cola', 'soda'],\n",
        "                     'ayran yogurt': ['dairy', 'mideast'],\n",
        "                     'beer': ['alcohol', 'soda'], 'juice': ['fruit', 'juice'],\n",
        "                     'wine': ['alcohol', 'wine'],\n",
        "                     'green tea': ['tea', 'asian', 'water'],\n",
        "                     'sake': ['alcohol', 'asian'],\n",
        "                     'tea': ['asian', 'tea', 'water'],\n",
        "                     'cocktail': ['alcohol', 'liquor'],\n",
        "                     'bubble tea': ['tea', 'juice', 'dairy'],\n",
        "                     'crush': ['fruit', 'soda'],\n",
        "                     'canada dry': ['ginger', 'soda'],\n",
        "                     'red wine': ['alcohol', 'wine'],\n",
        "                     'coco cola': ['cola', 'soda'],\n",
        "                     'alcohol': ['alcohol', 'liquor'],\n",
        "                     'chocolate milk': ['dairy', 'juice'],\n",
        "                     'fanta': ['fruit', 'soda'], 'soft drink': ['soda'],\n",
        "                     'sparkling water': ['water', 'soda'],\n",
        "                     'mountain dew': ['fruit', 'soda'], 'no drink': ['none'],\n",
        "                     'alcoholic drink': ['alcohol', 'liquor'],\n",
        "                     'milk': ['dairy'], 'coke': ['cola', 'soda'],\n",
        "                     'soda': ['soda'], 'cola': ['cola', 'soda'],\n",
        "                     'mango lassi': ['fruit', 'dairy'],\n",
        "                     'soy sauce': ['soy_sauce', 'asian']}\n",
        "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
        "    drinks = []\n",
        "    if detect_stopwords(text):\n",
        "        querywords = text.split()\n",
        "        resultwords = [word for word in querywords if\n",
        "                       word.lower() not in stop_words]\n",
        "        text = ' '.join(resultwords)\n",
        "        drinks = [common_drinks[drink] for drink in list(common_drinks) if\n",
        "                  drink in text]\n",
        "    if drinks:\n",
        "        drinks = [x for xs in drinks for x in xs]\n",
        "        drinks = list(set(drinks))\n",
        "        return ','.join(drinks)\n",
        "    else:\n",
        "        sims = [0] * len(common_drinks)\n",
        "        for i in range(len(common_drinks)):\n",
        "            sims[i] = jaro_winkler_similarity(text,\n",
        "                                              list(common_drinks)[i])\n",
        "            search = common_drinks[list(common_drinks)[np.argmax(sims)]]\n",
        "        return ','.join(search)\n",
        "\n",
        "\n",
        "def clean_drink(text: str) -> str:\n",
        "    if not text:\n",
        "        return None\n",
        "    drinks = detect_drinks(text)\n",
        "    if drinks:\n",
        "        return drinks\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def clean_movie(text: str) -> list:\n",
        "    movies = detect_movie(text)\n",
        "    if len(movies) > 0:\n",
        "        return [movies[\"title\"].values[0], movies[\"genre_ids\"].values[0]]\n",
        "    else:\n",
        "        return [\"none\", \"\"]\n",
        "\n",
        "\n",
        "def final_cleaning(fn: str):\n",
        "    df = pd.read_csv(\"/cleaned_data_combined_modified.csv\")\n",
        "    df = df.rename(columns={\n",
        "        \"Q1: From a scale 1 to 5, how complex is it to make this food? (Where 1 is the most simple, and 5 is the most complex)\": 'food_complexity',\n",
        "        'Q2: How many ingredients would you expect this food item to contain?': 'num_ingredients',\n",
        "        'Q3: In what setting would you expect this food to be served? Please check all that apply': 'serving_setting',\n",
        "        'Q4: How much would you expect to pay for one serving of this food item?': 'expected_cost',\n",
        "        'Q5: What movie do you think of when thinking of this food item?': 'related_movie',\n",
        "        'Q6: What drink would you pair with this food item?': 'paired_drink',\n",
        "        \"Q7: When you think about this food item, who does it remind you of?\": 'associated_people',\n",
        "        'Q8: How much hot sauce would you add to this food item?': 'hot_sauce_level'})\n",
        "    df[\"movie_genres\"] = ''\n",
        "    df[\"num_ingredients\"] = df[\"num_ingredients\"].apply(clean_ingredients)\n",
        "    df[\"expected_cost\"] = df[\"expected_cost\"].apply(clean_cost)\n",
        "    df[\"paired_drink\"] = df[\"paired_drink\"].apply(clean_drink)\n",
        "    df[\"related_movie\"] = df[\"related_movie\"].apply(change_movie)\n",
        "    cleaned_movies = df[\"related_movie\"].apply(clean_movie).to_list()\n",
        "    df[[\"related_movie\", \"movie_genres\"]] = pd.DataFrame(cleaned_movies,\n",
        "                                                         index=df.index)\n",
        "    df.hot_sauce_level = df.hot_sauce_level.fillna('none')\n",
        "    df = df.fillna(0)\n",
        "\n",
        "    # Convert categorical labels to indicators\n",
        "\n",
        "    x = df.drop(columns=[col for col in df.columns if col.startswith(\"Label\")])\n",
        "    y_ = df[\"Label\"]\n",
        "    split_fets = [\"serving_setting\", \"paired_drink\", \"associated_people\",\n",
        "                  \"movie_genres\"]\n",
        "    for fet in split_fets:\n",
        "        x_dumb = x[fet].str.get_dummies(sep=\",\")\n",
        "        x_dumb = x_dumb.add_prefix((fet[fet.index('_') + 1:] + \"_\"), axis=1)\n",
        "        x = pd.concat([x, x_dumb], axis=1)\n",
        "    x = pd.get_dummies(x, columns=[\"related_movie\", \"hot_sauce_level\"],\n",
        "                       prefix=[\"movie\", \"hot\"])\n",
        "\n",
        "    x_ = x.drop(columns=[col for col in x.columns\n",
        "                         if col in [\"serving_setting\", \"paired_drink\",\n",
        "                                    \"associated_people\", \"related_movie\",\n",
        "                                    \"movie_genres\"]])\n",
        "    df = pd.concat([x_, y_], axis=1)\n",
        "    fets = pd.read_csv(\"/clean_results_final.csv\")\n",
        "    learned_fets = list(fets)\n",
        "    df = pd.concat([fets, df], axis=0, join=\"outer\", sort=False)\n",
        "    df = df.drop(columns=[col for col in df.columns\n",
        "                          if col not in learned_fets])\n",
        "    df = df[1644:]\n",
        "\n",
        "\n",
        "\n",
        "    df.to_csv(\"/clean_results_test.csv\")\n",
        "    df.drop(columns=[col for col in df.columns if col.startswith('Unnamed') or col == 'id'], inplace=True)\n",
        "    X = df.drop(columns=[col for col in df.columns if col.startswith(\"Label\")])\n",
        "    y = df[\"Label\"].values\n",
        "    print(X.shape)\n",
        "    print(y.shape)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "# --- Helper Functions for Splitting ---\n",
        "\n",
        "def gini_impurity(y):\n",
        "    \"\"\"Calculate the Gini impurity for a list/array of labels.\"\"\"\n",
        "    m = len(y)\n",
        "    if m == 0:\n",
        "        return 0\n",
        "    counts = {}\n",
        "    for label in y:\n",
        "        counts[label] = counts.get(label, 0) + 1\n",
        "    impurity = 1.0 - sum((count / m) ** 2 for count in counts.values())\n",
        "    return impurity\n",
        "\n",
        "\n",
        "def gini_index(y_left, y_right):\n",
        "    \"\"\"Calculate the weighted Gini index for a split.\"\"\"\n",
        "    m = len(y_left) + len(y_right)\n",
        "    return (len(y_left) / m) * gini_impurity(y_left) + (\n",
        "                len(y_right) / m) * gini_impurity(y_right)\n",
        "\n",
        "\n",
        "def best_split(X, y, feature_indices):\n",
        "    \"\"\"For a given set of features, find the best feature and threshold that yields the lowest Gini impurity.\"\"\"\n",
        "    best_feature, best_threshold, best_gini = None, None, float('inf')\n",
        "    for feature in feature_indices:\n",
        "        values = X[:, feature]\n",
        "        for threshold in np.unique(values):\n",
        "            left_indices = values <= threshold\n",
        "            right_indices = values > threshold\n",
        "            if sum(left_indices) == 0 or sum(right_indices) == 0:\n",
        "                continue\n",
        "            y_left = y[left_indices]\n",
        "            y_right = y[right_indices]\n",
        "            current_gini = gini_index(y_left, y_right)\n",
        "            if current_gini < best_gini:\n",
        "                best_gini = current_gini\n",
        "                best_feature = feature\n",
        "                best_threshold = threshold\n",
        "    return best_feature, best_threshold, best_gini\n",
        "\n",
        "\n",
        "def most_common_label(y):\n",
        "    \"\"\"Return the most common label from an array.\"\"\"\n",
        "    counts = {}\n",
        "    for label in y:\n",
        "        counts[label] = counts.get(label, 0) + 1\n",
        "    return max(counts, key=counts.get)\n",
        "\n",
        "\n",
        "# --- Decision Tree Implementation ---\n",
        "class Node:\n",
        "    def __init__(self, feature_index=None, threshold=None, left=None,\n",
        "                 right=None, *, value=None):\n",
        "        self.feature_index = feature_index  # index of feature used for split\n",
        "        self.threshold = threshold  # threshold value for the split\n",
        "        self.left = left  # left subtree\n",
        "        self.right = right  # right subtree\n",
        "        self.value = value  # class label for leaf node\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, max_features=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_features = max_features  # number of features to consider at each split\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n_features = X.shape[1]\n",
        "        if self.max_features is None:\n",
        "            self.max_features = self.n_features\n",
        "        self.root = self._build_tree(X, y, depth=0)\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        if len(set(y)) == 1:\n",
        "            return Node(value=y[0])\n",
        "        if len(y) < self.min_samples_split or (\n",
        "                self.max_depth is not None and depth >= self.max_depth):\n",
        "            return Node(value=most_common_label(y))\n",
        "        features = random.sample(range(self.n_features), self.max_features)\n",
        "        feature_index, threshold, _ = best_split(X, y, features)\n",
        "        if feature_index is None:\n",
        "            return Node(value=most_common_label(y))\n",
        "        left_indices = X[:, feature_index] <= threshold\n",
        "        right_indices = X[:, feature_index] > threshold\n",
        "        left_node = self._build_tree(X[left_indices], y[left_indices],\n",
        "                                     depth + 1)\n",
        "        right_node = self._build_tree(X[right_indices], y[right_indices],\n",
        "                                      depth + 1)\n",
        "        return Node(feature_index=feature_index, threshold=threshold,\n",
        "                    left=left_node, right=right_node)\n",
        "\n",
        "    def predict_one(self, x):\n",
        "        node = self.root\n",
        "        while node.value is None:\n",
        "            node = node.left if x[\n",
        "                                    node.feature_index] <= node.threshold else node.right\n",
        "        return node.value\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self.predict_one(x) for x in X])\n",
        "\n",
        "\n",
        "# --- Random Forest Implementation ---\n",
        "class RandomForest:\n",
        "    def __init__(self, n_estimators=10, max_depth=None, min_samples_split=2,\n",
        "                 max_features=None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_features = max_features\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.trees = []\n",
        "        n_samples = X.shape[0]\n",
        "        for _ in range(self.n_estimators):\n",
        "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
        "            X_sample = X[indices]\n",
        "            y_sample = y[indices]\n",
        "            tree = DecisionTree(max_depth=self.max_depth,\n",
        "                                min_samples_split=self.min_samples_split,\n",
        "                                max_features=self.max_features)\n",
        "            tree.fit(X_sample, y_sample)\n",
        "            self.trees.append(tree)\n",
        "\n",
        "    def predict(self, X):\n",
        "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
        "        return np.array(\n",
        "            [most_common_label(tree_preds[:, i]) for i in range(X.shape[0])])\n",
        "\n",
        "\n",
        "# --- Prediction Process ---\n",
        "def predict_all(filename):\n",
        "    start_time = time.time()  # Start measuring time\n",
        "\n",
        "    df = pd.read_csv(\"/clean_results_final.csv\")\n",
        "    df.drop(columns=[col for col in df.columns if\n",
        "                     col.startswith('Unnamed') or col == 'id'], inplace=True)\n",
        "    df = df.fillna(0)\n",
        "\n",
        "    target_column = 'Label'\n",
        "    y_series = df[target_column]\n",
        "    df.drop(columns=[col for col in df.columns if col.startswith('Label')],\n",
        "            inplace=True)\n",
        "    if y_series.dtype == 'O':\n",
        "        unique_labels = np.unique(y_series)\n",
        "        label_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "        y_series = y_series.map(label_map)\n",
        "\n",
        "    X_train, y_train = df.values, y_series.values\n",
        "    np.random.seed(42)\n",
        "\n",
        "    \"\"\"\n",
        "    indices = np.random.permutation(len(y))\n",
        "    train_size = int(0.7 * len(y))\n",
        "    val_size = int(0.25 * len(y))\n",
        "    train_idx = indices[:train_size]\n",
        "    val_idx = indices[train_size:train_size+val_size]\n",
        "    test_idx = indices[train_size+val_size:]\n",
        "    X_train, X_val, X_test = X[train_idx], X[val_idx], X[test_idx]\n",
        "    y_train, y_val, y_test = y[train_idx], y[val_idx], y[test_idx]\n",
        "    \"\"\"\n",
        "\n",
        "    rf_final = RandomForest(n_estimators=100, max_depth=15, min_samples_split=2,\n",
        "                            max_features=int(np.sqrt(X_train.shape[1])))\n",
        "    print(X_train.shape)\n",
        "    rf_final.fit(X_train, y_train)\n",
        "\n",
        "    # clean test data\n",
        "\n",
        "    X_test, y_test = final_cleaning(filename)\n",
        "    y_test_pred = rf_final.predict(X_test)\n",
        "    final_accuracy = np.mean(y_test_pred == y_test)\n",
        "\n",
        "    print(\"\\nFinal Test Accuracy:\", final_accuracy)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    filename = \"/clean_results_test.csv\"\n",
        "    predict_all(filename)\n"
      ]
    }
  ]
}